{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def remove_space_redundant(text):\n",
    "    words = text.split()\n",
    "    clean_text = \" \".join(words)\n",
    "    return clean_text\n",
    "\n",
    "def get_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def create_chunk_json(text, output_path):\n",
    "    content_between_chapters = re.findall(r\"(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.?)(.*?)(?=(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.? |$))\", text, re.DOTALL)\n",
    "    chapter_name = []\n",
    "    content_chapter = []\n",
    "    all_content_chapter = []    # extract từng chapter trước\n",
    "    for content_between_chapter in content_between_chapters:\n",
    "        chapter_name_temp = content_between_chapter[0].strip()\n",
    "        content_chapter_temp = content_between_chapter[1].strip()\n",
    "        chapter_name.append(chapter_name_temp.strip())\n",
    "        content_chapter.append(content_chapter_temp.strip())\n",
    "        all_content_chapter.append(content_between_chapter[0] + content_between_chapter[1])\n",
    "\n",
    "    chapter_title = []\n",
    "    rule_title = []\n",
    "    contents = []\n",
    "    regex_chapter = re.compile(r'(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.?)\\s*(.*)')\n",
    "    regex_rule = re.compile(r'(Điều \\d+\\.)(.*?)(?=(Điều \\d+\\. |$))', re.DOTALL)\n",
    "    for content_chap in all_content_chapter:\n",
    "        matches_chapter = regex_chapter.findall(content_chap)\n",
    "        matches_rule = regex_rule.findall(content_chap)\n",
    "        for match_rule in matches_rule:\n",
    "            for match_chapter in matches_chapter:\n",
    "                temp = match_chapter[0] + \"\\n\" + match_chapter[1]\n",
    "                chapter_title.append(temp.strip())\n",
    "            temp_title_rule = match_rule[0] + match_rule[1].split('\\n')[0].strip()\n",
    "            rule_title.append(temp_title_rule.strip())\n",
    "            temp_content_rule = remove_space_redundant(\" \".join(match_rule[1].split('\\n')[1:]).strip())\n",
    "            contents.append(temp_content_rule)\n",
    "\n",
    "    titles = []\n",
    "    for i in range(len(chapter_title)):\n",
    "        titles.append(\"Document Title\" + \"\\n\" + chapter_title[i] + \"\\n\" + rule_title[i])\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=64)\n",
    "\n",
    "    title_chunk, chunks = [], []\n",
    "    for i in range(len(contents)):\n",
    "        chunk = text_splitter.split_text(contents[i])\n",
    "\n",
    "        num = len(chunk)\n",
    "        for k in range(num):\n",
    "            title_chunk.append(titles[i])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    chunks = [item for chunk in chunks for item in chunk]\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(title_chunk)):\n",
    "        data.append({'title': title_chunk[i], 'context': chunks[i]})\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def process_pdf_file(event, context):\n",
    "    \"\"\"Triggered by a Pub/Sub message.\"\"\"\n",
    "    pubsub_message = base64.b64decode(event['data']).decode('utf-8')\n",
    "    message_data = json.loads(pubsub_message)\n",
    "\n",
    "    # Kiểm tra các khóa cần thiết trong thông điệp Pub/Sub\n",
    "    if 'bucket' not in message_data or 'name' not in message_data:\n",
    "        print(\"Missing 'bucket' or 'name' in Pub/Sub message\")\n",
    "        return\n",
    "\n",
    "    bucket_name = message_data['bucket']\n",
    "    file_name = message_data['name']\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    temp_pdf_path = f\"/tmp/{file_name}\"\n",
    "    temp_json_path = temp_pdf_path.replace('.pdf', '.json')\n",
    "\n",
    "    # Tải tệp PDF về từ GCS\n",
    "    blob.download_to_filename(temp_pdf_path)\n",
    "\n",
    "    # Trích xuất văn bản từ PDF\n",
    "    text = get_text_from_pdf(temp_pdf_path)\n",
    "\n",
    "    # Tạo JSON từ văn bản đã trích xuất\n",
    "    create_chunk_json(text, temp_json_path)\n",
    "\n",
    "    # Upload JSON trở lại GCS\n",
    "    output_blob = bucket.blob(temp_json_path.split('/')[-1])\n",
    "    output_blob.upload_from_filename(temp_json_path)\n",
    "\n",
    "    print(f\"Processed {file_name} and saved result to {temp_json_path.split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def remove_space_redundant(text):\n",
    "    words = text.split()\n",
    "    clean_text = \" \".join(words)\n",
    "    return clean_text\n",
    "\n",
    "def get_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def create_chunk_json(text):\n",
    "    content_between_chapters = re.findall(r\"(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.?)(.*?)(?=(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.? |$))\", text, re.DOTALL)\n",
    "    chapter_name = []\n",
    "    content_chapter = []\n",
    "    all_content_chapter = []    # extract từng chapter trước\n",
    "    for content_between_chapter in content_between_chapters:\n",
    "        chapter_name_temp = content_between_chapter[0].strip()\n",
    "        content_chapter_temp = content_between_chapter[1].strip()\n",
    "        chapter_name.append(chapter_name_temp.strip())\n",
    "        content_chapter.append(content_chapter_temp.strip())\n",
    "        all_content_chapter.append(content_between_chapter[0] + content_between_chapter[1])\n",
    "\n",
    "    chapter_title = []\n",
    "    rule_title = []\n",
    "    contents = []\n",
    "    regex_chapter = re.compile(r'(Chương \\b(?:I{1,3}(?:V?X?)?|VI{0,3}|XI{0,3}V?|XVI{0,3})\\b\\.?)\\s*(.*)')\n",
    "    regex_rule = re.compile(r'(Điều \\d+\\.)(.*?)(?=(Điều \\d+\\. |$))', re.DOTALL)\n",
    "    for content_chap in all_content_chapter:\n",
    "        matches_chapter = regex_chapter.findall(content_chap)\n",
    "        matches_rule = regex_rule.findall(content_chap)\n",
    "        for match_rule in matches_rule:\n",
    "            for match_chapter in matches_chapter:\n",
    "                temp = match_chapter[0] + \"\\n\" + match_chapter[1]\n",
    "                chapter_title.append(temp.strip())\n",
    "            temp_title_rule = match_rule[0] + match_rule[1].split('\\n')[0].strip()\n",
    "            rule_title.append(temp_title_rule.strip())\n",
    "            temp_content_rule = remove_space_redundant(\" \".join(match_rule[1].split('\\n')[1:]).strip())\n",
    "            contents.append(temp_content_rule)\n",
    "\n",
    "    titles = []\n",
    "    for i in range(len(chapter_title)):\n",
    "        titles.append(\"Document Title\" + \"\\n\" + chapter_title[i] + \"\\n\" + rule_title[i])\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=64)\n",
    "\n",
    "    title_chunk, chunks = [], []\n",
    "    for i in range(len(contents)):\n",
    "        chunk = text_splitter.split_text(contents[i])\n",
    "\n",
    "        num = len(chunk)\n",
    "        for k in range(num):\n",
    "            title_chunk.append(titles[i])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    chunks = [item for chunk in chunks for item in chunk]\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(title_chunk)):\n",
    "        data.append({'title': title_chunk[i], 'context': chunks[i]})\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_pdf_file(event, context):\n",
    "    \"\"\"Triggered by a Pub/Sub message.\"\"\"\n",
    "    pubsub_message = base64.b64decode(event['data']).decode('utf-8')\n",
    "    message_data = json.loads(pubsub_message)\n",
    "\n",
    "    # Kiểm tra các khóa cần thiết trong thông điệp Pub/Sub\n",
    "    if 'bucket' not in message_data or 'name' not in message_data:\n",
    "        print(\"Missing 'bucket' or 'name' in Pub/Sub message\")\n",
    "        return\n",
    "\n",
    "    bucket_name = message_data['bucket']\n",
    "    file_name = message_data['name']\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    temp_pdf_path = f\"/tmp/{file_name}\"\n",
    "    temp_json_path = temp_pdf_path.replace('.pdf', '.json')\n",
    "\n",
    "    # Tải tệp PDF về từ GCS\n",
    "    blob.download_to_filename(temp_pdf_path)\n",
    "\n",
    "    # Trích xuất văn bản từ PDF hiện tại và tạo JSON\n",
    "    text = get_text_from_pdf(temp_pdf_path)\n",
    "    current_file_json = create_chunk_json(text)\n",
    "\n",
    "    # Duyệt qua tất cả các tệp PDF khác trong bucket và tạo JSON\n",
    "    all_text = \"\"\n",
    "    blobs = bucket.list_blobs()\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.pdf') and blob.name != file_name:\n",
    "            temp_pdf_path = f\"/tmp/{blob.name}\"\n",
    "            blob.download_to_filename(temp_pdf_path)\n",
    "            text = get_text_from_pdf(temp_pdf_path)\n",
    "            all_text += text + \"\\n\"\n",
    "\n",
    "    other_files_json = create_chunk_json(all_text)\n",
    "\n",
    "    # Kết nối JSON từ file hiện tại và các file khác\n",
    "    combined_json = current_file_json + other_files_json\n",
    "\n",
    "    # Kiểm tra và xóa file all.json nếu tồn tại\n",
    "    if bucket.blob('all.json').exists():\n",
    "        bucket.blob('all.json').delete()\n",
    "\n",
    "    # Upload JSON kết hợp trở lại GCS với tên all.json\n",
    "    output_json_path = \"/tmp/all.json\"\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(combined_json, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    output_blob = bucket.blob('all.json')\n",
    "    output_blob.upload_from_filename(output_json_path)\n",
    "\n",
    "    print(f\"Processed {file_name} and combined result saved to all.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
